{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b37d4a4",
   "metadata": {},
   "source": [
    "This tutorial follows the samples and explanations of https://www.pinecone.io/learn/series/langchain/langchain-intro/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf8bf10",
   "metadata": {},
   "source": [
    "# Chapter 1. Introduction to LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc0061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49d7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI\n",
    "path = \"API_key.txt\"\n",
    "with open(path) as f:\n",
    "    API_key = f.readlines()[0]\n",
    "    \n",
    "os.environ['OPENAI_API_KEY'] = API_key\n",
    "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
    "os.environ['OPENAI_API_BASE'] = 'azure-openai-api-url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca862ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate builds the general structure for a traditional prompt\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template,\n",
    "                        input_variables = ['question'])\n",
    "\n",
    "question = \"Tell me about the band Ghost.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb039ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model\n",
    "llm = AzureOpenAI(deployment_name = \"text-davinci-003\",\n",
    "                  model_name = \"text-davinci-003\")\n",
    "\n",
    "# Run the prompt with the LLM\n",
    "llm_chain = LLMChain(prompt = prompt,\n",
    "                     llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64403834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ghost is a Swedish heavy metal band that was formed in Linköping, Sweden in 2008. They are known for their unique blend of heavy metal, hard rock, and doom metal with occult and horror-inspired lyrics. The band's musical style has been described as a mix of classic '70s and '80s hard rock and heavy metal with a modern touch. The band's lineup consists of vocalist Tobias Forge, the band's leader, and four additional musicians who use the stage names of Nameless Ghouls. Ghost has released three studio albums, two live albums, and a number of singles and EPs. Their most recent album, Prequelle, was released in 2018.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee58ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=' 6ft 4 inches is equal to 193.04 centimeters.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' LangChain is a decentralized language learning platform that uses blockchain technology to enable users to learn languages from native speakers and earn rewards for their language learning efforts. The platform connects language learners with native speakers who can provide personalized language lessons. Learners can earn rewards in the form of cryptocurrency for completing lessons and helping others learn languages.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' Tolkien wrote three books related to The Lord of the Rings: The Fellowship of the Ring, The Two Towers, and The Return of the King.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' As of July 2020, there are 898 known species of Pokémon.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 187, 'prompt_tokens': 68, 'completion_tokens': 119}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('ed440486-7655-49e3-b6e8-9e715b3d07a4')), RunInfo(run_id=UUID('2315705e-4363-447e-a940-23b0aaf14591')), RunInfo(run_id=UUID('2513ac49-dfcf-4370-9ad4-8853d4d130c6')), RunInfo(run_id=UUID('19202ce6-4351-4aed-b7e2-17705380a358'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run multiple questions\n",
    "qs = [{'question': \"If I'm 6ft 4 inches, how tall am I in centimeters?\"},\n",
    "      {'question': 'What is LangChain?'},\n",
    "      {'question': 'How many books does Tolkien wrote related to the Lord of the Rings?'},\n",
    "      {'question': 'How many pokemons exist?'}]\n",
    "\n",
    "llm_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b1f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6ft 4 inches is approximately 193 centimeters.\n",
      "LangChain is an online language learning platform that uses AI and NLP technology to help users learn a language in an interactive and fun way.\n",
      "\n",
      "J.R.R. Tolkien wrote three books related to the Lord of the Rings: The Fellowship of the Ring, The Two Towers, and The Return of the King.\n",
      "\n",
      "There are currently 809 different species of Pokemon.\n"
     ]
    }
   ],
   "source": [
    "# Get just the answers\n",
    "qs_str = [\"If I'm 6ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "          'What is LangChain?\\n' +\n",
    "          'How many books does Tolkien wrote related to the Lord of the Rings?\\n' +\n",
    "          'How many pokemons exist?']\n",
    "\n",
    "print(llm_chain.run(qs_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdec445",
   "metadata": {},
   "source": [
    "# Chapter 2. Prompt Templates and the Art of Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61d0ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain import PromptTemplate, FewShotPromptTemplate, LLMChain\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d290f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI\n",
    "path = \"API_key.txt\"\n",
    "with open(path) as f:\n",
    "    API_key = f.readlines()[0]\n",
    "    \n",
    "os.environ['OPENAI_API_KEY'] = API_key\n",
    "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
    "os.environ['OPENAI_API_BASE'] = 'azure-openai-api-url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eff1f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below.\n",
    "If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly useful for developers\n",
    "building NLP enabled applications. These models can be accessed via Hugging Face's `transformers`\n",
    "library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: which libraries and omdel providers offer LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f006170",
   "metadata": {},
   "source": [
    "You can run the prompt directly after defining the LLM model you want to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2f79270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
     ]
    }
   ],
   "source": [
    "# Specify the model\n",
    "llm = AzureOpenAI(deployment_name = \"text-davinci-003\",\n",
    "                  model_name = \"text-davinci-003\")\n",
    "\n",
    "# Run the prompt with the LLM\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24667731",
   "metadata": {},
   "source": [
    "In case you have prompts with **dynamic inputs**, the best approach is to use **prompt templates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f50a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based on the context below.\n",
    "If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly useful for developers\n",
    "building NLP enabled applications. These models can be accessed via Hugging Face's `transformers`\n",
    "library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2b68c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(input_variables = [\"query\"],\n",
    "                                 template = template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad88821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below.\n",
      "If the question cannot be answered using the information provided answer with \"I don't know\".\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
      "Their superior performance over smaller models has made them incredibly useful for developers\n",
      "building NLP enabled applications. These models can be accessed via Hugging Face's `transformers`\n",
      "library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
      "\n",
      "Question: Which libraries and model providers offer LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# Modify the prompt template based on the dynamic input (the user question)\n",
    "print(prompt_template.format(query = \"Which libraries and model providers offer LLMs?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c04ec282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
     ]
    }
   ],
   "source": [
    "# Run directly the prompt\n",
    "print(llm(prompt_template.format(\n",
    "            query = \"Which libraries and model providers offer LLMs?\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb0784",
   "metadata": {},
   "source": [
    "## Few Shot Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee70d868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To be the best version of yourself that you can be!\n"
     ]
    }
   ],
   "source": [
    "# Use of few-shot samples for improving the prompt\n",
    "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions.\n",
    "Here are some examples:\n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "llm.temperature = 1.0\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6960f970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The meaning of life is to find your own purpose and live it to the fullest!\n"
     ]
    }
   ],
   "source": [
    "# Giving more examples of input-output improves the quality of response\n",
    "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions.\n",
    "Here are some examples:\n",
    "\n",
    "User: How are you?\n",
    "AI: I can't complain but sometimes I still do.\n",
    "\n",
    "User: What time is it?\n",
    "AI: It's time to get a watch.\n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "llm.temperature = 1.0\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "181cdbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formalize this process with a specific template\n",
    "examples = [{\"query\": 'How are you?',\n",
    "             \"answer\": \"I can't complain but sometimes I still do.\"},\n",
    "            {\"query\": \"What time is it?\",\n",
    "             \"answer\": \"It's time to get a watch.\"}]\n",
    "\n",
    "example_template = \"\"\"User: {query}\n",
    "AI: {answer}\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables = ['query', 'answer'],\n",
    "                                template = example_template)\n",
    "\n",
    "prefix = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions.\n",
    "Here are some examples:\"\"\"\n",
    "\n",
    "suffix = \"\"\"User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(examples = examples,\n",
    "                                                 example_prompt = example_prompt,\n",
    "                                                 prefix = prefix,\n",
    "                                                 suffix = suffix,\n",
    "                                                 input_variables = ['query'],\n",
    "                                                 example_separator = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b259d844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a conversation with an AI assistant.\n",
      "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions.\n",
      "Here are some examples:\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57d60fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the number of examples for the prompt\n",
    "examples = [{\"query\": \"How are you?\",\n",
    "             \"answer\": \"I can't complain but sometimes I still do.\"},\n",
    "            {\"query\": \"What time is it?\",\n",
    "             \"answer\": \"It's time to get a watch.\"},\n",
    "            {\"query\": \"What is the meaning of life?\",\n",
    "             \"answer\": \"42!\"},\n",
    "            {\"query\": \"What is the weather like today?\",\n",
    "             \"answer\": \"Cloudy with a chance of memes.\"},\n",
    "            {\"query\": \"What is your favorite movie?\",\n",
    "             \"answer\": \"Terminator\"},\n",
    "            {\"query\": \"Who is your best friend?\",\n",
    "             \"answer\": \"Siri. We have spirited debates about the meaning of life.\"},\n",
    "            {\"query\": \"What should I do today?\",\n",
    "             \"answer\": \"Stop talking to chatbots on the internet and go outside.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbf2a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = LengthBasedExampleSelector(examples = examples,\n",
    "                                              example_prompt = example_prompt,\n",
    "                                              max_length = 50) #Maximum number of words, not tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffae4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_prompt_template = FewShotPromptTemplate(example_selector = example_selector,\n",
    "                                                example_prompt = example_prompt,\n",
    "                                                prefix = prefix,\n",
    "                                                suffix = suffix,\n",
    "                                                input_variables = ['query'],\n",
    "                                                example_separator = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7baac1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a conversation with an AI assistant.\n",
      "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions.\n",
      "Here are some examples:\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "User: What is the meaning of life?\n",
      "AI: 42!\n",
      "User: How do birds fly?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"How do birds fly?\"\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcfdc865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a conversation with an AI assistant.\n",
      "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions.\n",
      "Here are some examples:\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "User: If I am in America, and I want to call someone in another country, I'm\n",
      "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
      "what is the best way to do that?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "# longer questions limit the given example questions\n",
    "query = \"\"\"If I am in America, and I want to call someone in another country, I'm\n",
    "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
    "what is the best way to do that?\"\"\"\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed87cce",
   "metadata": {},
   "source": [
    "# Chapter 3. Building Composable Pipelines with Chains\n",
    "\n",
    "A chain is basically a pipeline that processes an input by using a specific combination of primitives. Intuitively, it can be thought of as a 'step' that performs a certain set of operations on an input and returns the result. They can be anything from a prompt-based pass through a LLM to applying a Python function to an text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea7ea910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import inspect\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffc2b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI\n",
    "path = \"API_key.txt\"\n",
    "with open(path) as f:\n",
    "    API_key = f.readlines()[0]\n",
    "    \n",
    "os.environ['OPENAI_API_KEY'] = API_key\n",
    "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
    "os.environ['OPENAI_API_BASE'] = 'azure-openai-api-url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a507b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model\n",
    "llm = AzureOpenAI(deployment_name = \"text-davinci-003\",\n",
    "                  model_name = \"text-davinci-003\",\n",
    "                  temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e742cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving used tokens\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f\"Spent a total of {cb.total_tokens} tokens\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0780b847",
   "metadata": {},
   "source": [
    "## Utility Chains\n",
    "\n",
    "Chains that are usually used to extract a specific answer from a LLM with a very narrow purpose and are ready to be used out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be8e8740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\GReyes15\\Anaconda3\\lib\\site-packages\\langchain\\chains\\llm_math\\base.py:56: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is 13 raised to the 0.3432 power?\u001b[32;1m\u001b[1;3m\n",
      "```text\n",
      "13**0.3432\n",
      "```\n",
      "...numexpr.evaluate(\"13**0.3432\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 267 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 2.4116004626599237'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the LLM to do math\n",
    "llm_math = LLMMathChain(llm = llm,\n",
    "                        verbose = True) #Show the different steps that are run on the chain\n",
    "count_tokens(llm_math,\n",
    "             \"What is 13 raised to the 0.3432 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "041b3dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the chain prompt executed above\n",
    "print(llm_math.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22673580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(\n",
      "        self,\n",
      "        inputs: Dict[str, str],\n",
      "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
      "    ) -> Dict[str, str]:\n",
      "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
      "        _run_manager.on_text(inputs[self.input_key])\n",
      "        llm_output = self.llm_chain.predict(\n",
      "            question=inputs[self.input_key],\n",
      "            stop=[\"```output\"],\n",
      "            callbacks=_run_manager.get_child(),\n",
      "        )\n",
      "        return self._process_llm_result(llm_output, _run_manager)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(llm_math._call))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a185e",
   "metadata": {},
   "source": [
    "## Generic Chains\n",
    "\n",
    "Chains that are used as building blocks for other chains but cannot be used out of the box on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7585e",
   "metadata": {},
   "source": [
    "First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc136961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs['text']\n",
    "    \n",
    "    #Replace multiple new lines and multiple spaces with a single one\n",
    "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    \n",
    "    return {'output_text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8182df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_extra_spaces_chain = TransformChain(input_variables = ['text'],\n",
    "                                          output_variables = ['output_text'],\n",
    "                                          transform = transform_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "850ad31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A random text with some irregular spacing.\\n Another one here as well.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_extra_spaces_chain.run(\"A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d4e1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previous TransformChain to paraphrase input texts with LLMs\n",
    "template = \"\"\"Paraphrase this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "In the style of a {style}.\n",
    "\n",
    "Paraphrase: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables = ['style', 'output_text'],\n",
    "                        template = template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "affcd471",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_paraphrase_chain = LLMChain(llm = llm,\n",
    "                                  prompt = prompt,\n",
    "                                  output_key = 'final_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1249393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(chains = [clean_extra_spaces_chain,\n",
    "                                             style_paraphrase_chain],\n",
    "                                   input_variables = ['text', 'style'],\n",
    "                                   output_variables = ['final_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f522b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Chains allow us to combine multiple \n",
    "\n",
    "\n",
    "components together to create a single, coherent application. \n",
    "\n",
    "For example, we can create a chain that takes user input,       format it with a PromptTemplate, \n",
    "\n",
    "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by \n",
    "\n",
    "\n",
    "combining chains with other components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15ffcd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 163 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nChains let us link up multiple pieces to make one dope app. Like, we can take user input, style it up with a PromptTemplate, then pass it to an LLM. We can get even more creative by combining multiple chains or mixin' chains with other components.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(sequential_chain,\n",
    "             {'text': input_text,\n",
    "              'style': 'a 90s rapper'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b03bf",
   "metadata": {},
   "source": [
    "# Chapter 4. Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d55f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
    "                                                  ConversationSummaryMemory,\n",
    "                                                  ConversationBufferWindowMemory,\n",
    "                                                  ConversationSummaryBufferMemory)\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7d8d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI\n",
    "path = \"API_key.txt\"\n",
    "with open(path) as f:\n",
    "    API_key = f.readlines()[0]\n",
    "    \n",
    "os.environ['OPENAI_API_KEY'] = API_key\n",
    "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
    "os.environ['OPENAI_API_BASE'] = 'azure-openai-api-url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1485f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model\n",
    "llm = AzureOpenAI(deployment_name = \"text-davinci-003\",\n",
    "                  model_name = \"text-davinci-003\",\n",
    "                  temperature = 0)\n",
    "\n",
    "# Initialize the conversation chain\n",
    "conversation = ConversationChain(llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d14ca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "# Check prompt template of conversation template\n",
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be35f2b6",
   "metadata": {},
   "source": [
    "## Conversation Buffer Memory\n",
    "\n",
    "Is the most straightforward conversational memory in LangChain. The raw input of the past conversation between the human and AI is passed — in its raw form — to the {history} parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad995223",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_buf = ConversationChain(llm = llm,\n",
    "                                     memory = ConversationBufferMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49376c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hello AI!',\n",
       " 'history': '',\n",
       " 'response': \" Hi there! It's nice to meet you. How can I help you today?\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buf(\"Hello AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c6d57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "083c7a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 143 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Interesting! That sounds like a fascinating topic. I'm not sure I know much about it, but I'm sure I can help you find some resources. What kind of resources are you looking for?\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_buf,\n",
    "             \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e7a4733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 247 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hmm, that's a great question. I'm not sure I have a definitive answer, but I can think of a few possibilities. For example, you could explore the potential of using large language models to generate more accurate natural language processing results, or you could look into using them to create more accurate machine translation systems. You could also explore the potential of using them to create more accurate text summarization systems.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_buf,\n",
    "             \"I just want to analyze the different possibilities. What can you think of?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6044369e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 364 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's a great question. There are a few different types of data sources that could be used to give context to a large language model. For example, you could use text corpora, which are collections of text documents that can be used to train the model. You could also use structured data sources, such as databases or spreadsheets, to provide additional context. Additionally, you could use unstructured data sources, such as audio or video recordings, to provide additional context.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_buf, \n",
    "             \"Which data source types could be used to give context to the model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b01c7cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello AI!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "AI:  Interesting! That sounds like a fascinating topic. I'm not sure I know much about it, but I'm sure I can help you find some resources. What kind of resources are you looking for?\n",
      "Human: I just want to analyze the different possibilities. What can you think of?\n",
      "AI:  Hmm, that's a great question. I'm not sure I have a definitive answer, but I can think of a few possibilities. For example, you could explore the potential of using large language models to generate more accurate natural language processing results, or you could look into using them to create more accurate machine translation systems. You could also explore the potential of using them to create more accurate text summarization systems.\n",
      "Human: Which data source types could be used to give context to the model?\n",
      "AI:  That's a great question. There are a few different types of data sources that could be used to give context to a large language model. For example, you could use text corpora, which are collections of text documents that can be used to train the model. You could also use structured data sources, such as databases or spreadsheets, to provide additional context. Additionally, you could use unstructured data sources, such as audio or video recordings, to provide additional context.\n"
     ]
    }
   ],
   "source": [
    "# Check the buffer memory\n",
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994b4b7",
   "metadata": {},
   "source": [
    "## Conversation Summary Memory\n",
    "\n",
    "This form of memory summarizes the conversation history before it is passed to the {history} parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d251bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm,\n",
    "                                 memory=ConversationSummaryMemory(llm=llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ff92505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4daf2008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 283 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can use conversation(\"Good morning AI!\") without checking on total tokens\n",
    "count_tokens(conversation, \n",
    "             \"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7751d78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 420 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That sounds like an interesting project! I'm familiar with Large Language Models, but I'm not sure how they could be integrated with external knowledge. Could you tell me more about what you have in mind?\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation,\n",
    "             \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2fb8e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 629 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I can think of a few possibilities. One is to use a large language model to generate text that is based on external knowledge. This could be used to generate stories, articles, or other types of content. Another possibility is to use the large language model to generate questions and answers based on external knowledge. This could be used to create a knowledge base or to answer questions posed by users. Finally, the large language model could be used to generate natural language responses to user queries.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation,\n",
    "             \"I just want to analyze the different possibilities. What can you think of?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe7c0b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human greeted the AI and the AI responded with a greeting and asked how it could help. The human then expressed interest in exploring the potential of integrating Large Language Models with external knowledge, to which the AI responded positively and asked for more information. The AI then suggested a few possibilities, such as using the large language model to generate text based on external knowledge, generate questions and answers based on external knowledge, or generate natural language responses to user queries.\n"
     ]
    }
   ],
   "source": [
    "# Check the summaries given to the chain\n",
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e808125",
   "metadata": {},
   "source": [
    "## Conversation Buffer Window Memory\n",
    "\n",
    "Acts in the same way as our earlier “buffer memory” but adds a window to the memory. Meaning that we only keep a given number of past interactions before “forgetting” them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b1b1f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm = llm,\n",
    "                                 memory = ConversationBufferWindowMemory(k = 1)) #The model remembers the k latest interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b94e261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 85 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation,\n",
    "             \"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb61f17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 178 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation,\n",
    "             \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "119e5318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 233 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' There are many possibilities for integrating Large Language Models with external knowledge. For example, you could use external knowledge to provide additional context to the model, or to provide additional training data. You could also use external knowledge to help the model better understand the context of a given text, or to help it generate more accurate results.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation,\n",
    "             \"I just want to analyze the different possibilities. What can you think of?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d003f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 169 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Your aim is to analyze the different possibilities for integrating Large Language Models with external knowledge.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation, \n",
    "             \"What is my aim again?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "35c16f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the buffer history\n",
    "bufw_history = conversation.memory.load_memory_variables(inputs = [])['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "898e33ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my aim again?\n",
      "AI:  Your aim is to analyze the different possibilities for integrating Large Language Models with external knowledge.\n"
     ]
    }
   ],
   "source": [
    "print(bufw_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000390b2",
   "metadata": {},
   "source": [
    "## Conversation Summary Buffer Memory\n",
    "\n",
    "It's a combination of the summary buffer and the conversation buffer window. Summarizes the earliest interactions while maintaining the {max_token_limit} most recent tokens of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f297e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_sum_bufw = ConversationChain(llm = llm,\n",
    "                                          memory = ConversationSummaryBufferMemory(llm = llm,\n",
    "                                                                                   max_token_limit = 650))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d2144",
   "metadata": {},
   "source": [
    "# Chapter 5. Retrieval Augmentation (Preventing Hallucination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8134ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    datasets==2.12.0 \\\n",
    "    apache_beam \\\n",
    "    mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5597b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import pinecone\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5fd16472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI\n",
    "path = \"API_key.txt\"\n",
    "with open(path) as f:\n",
    "    API_key = f.readlines()[0]\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = API_key\n",
    "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
    "os.environ['OPENAI_API_BASE'] = 'azure-openai-api-url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6cf40991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikipedia/20220301.simple to D:/Users/GReyes15/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bf379aec054113a909c984af350482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bd57c26e324e4485d4b2ef55f5b619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/235M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikipedia downloaded and prepared to D:/Users/GReyes15/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download knowledge base dataset\n",
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", split='train[:1000]')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dfabf70b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13',\n",
       " 'url': 'https://simple.wikipedia.org/wiki/Alan%20Turing',\n",
       " 'title': 'Alan Turing',\n",
       " 'text': 'Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English mathematician and computer scientist. He was born in Maida Vale, London.\\n\\nEarly life and family \\nAlan Turing was born in Maida Vale, London on 23 June 1912. His father was part of a family of merchants from Scotland. His mother, Ethel Sara, was the daughter of an engineer.\\n\\nEducation \\nTuring went to St. Michael\\'s, a school at 20 Charles Road, St Leonards-on-sea, when he was five years old.\\n\"This is only a foretaste of what is to come, and only the shadow of what is going to be.” – Alan Turing.\\n\\nThe Stoney family were once prominent landlords, here in North Tipperary. His mother Ethel Sara Stoney (1881–1976) was daughter of Edward Waller Stoney (Borrisokane, North Tipperary) and Sarah Crawford (Cartron Abbey, Co. Longford); Protestant Anglo-Irish gentry.\\n\\nEducated in Dublin at Alexandra School and College; on October 1st 1907 she married Julius Mathison Turing, latter son of Reverend John Robert Turing and Fanny Boyd, in Dublin. Born on June 23rd 1912, Alan Turing would go on to be regarded as one of the greatest figures of the twentieth century.\\n\\nA brilliant mathematician and cryptographer Alan was to become the founder of modern-day computer science and artificial intelligence; designing a machine at Bletchley Park to break secret Enigma encrypted messages used by the Nazi German war machine to protect sensitive commercial, diplomatic and military communications during World War 2. Thus, Turing made the single biggest contribution to the Allied victory in the war against Nazi Germany, possibly saving the lives of an estimated 2 million people, through his effort in shortening World War II.\\n\\nIn 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\\n\\nAlas, Turing accidentally or otherwise lost his life in 1954, having been subjected by a British court to chemical castration, thus avoiding a custodial sentence. He is known to have ended his life at the age of 41 years, by eating an apple laced with cyanide.\\n\\nCareer \\nTuring was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\\n\\nTuring was interested in artificial intelligence. He proposed the Turing test, to say when a machine could be called \"intelligent\". A computer could be said to \"think\" if a human talking with it could not tell it was a machine.\\n\\nDuring World War II, Turing worked with others to break German ciphers (secret messages). He  worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain\\'s codebreaking centre that produced Ultra intelligence.\\nUsing cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.\\n\\nFrom 1945 to 1947, Turing worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory. He presented a paper on 19 February 1946. That paper was \"the first detailed design of a stored-program computer\". Although it was possible to build ACE, there were delays in starting the project. In late 1947 he returned to Cambridge for a sabbatical year. While he was at Cambridge, the Pilot ACE was built without him. It ran its first program on 10\\xa0May 1950.\\n\\nPrivate life \\nTuring was a homosexual man. In 1952, he admitted having had sex with a man in England. At that time, homosexual acts were illegal. Turing was convicted. He had to choose between going to jail and taking hormones to lower his sex drive. He decided to take the hormones. After his punishment, he became impotent. He also grew breasts.\\n\\nIn May 2012, a private member\\'s bill was put before the House of Lords to grant Turing a statutory pardon. In July 2013, the government supported it. A royal pardon was granted on 24 December 2013.\\n\\nDeath \\nIn 1954, Turing died from cyanide poisoning. The cyanide came from either an apple which was poisoned with cyanide, or from water that had cyanide in it. The reason for the confusion is that the police never tested the apple for cyanide. It is also suspected that he committed suicide.\\n\\nThe treatment forced on him is now believed to be very wrong. It is against medical ethics and international laws of human rights. In August 2009, a petition asking the British Government to apologise to Turing for punishing him for being a homosexual was started. The petition received thousands of signatures. Prime Minister Gordon Brown acknowledged the petition. He called Turing\\'s treatment \"appalling\".\\n\\nReferences\\n\\nOther websites \\nJack Copeland 2012. Alan Turing: The codebreaker who saved \\'millions of lives\\'. BBC News / Technology \\n\\nEnglish computer scientists\\nEnglish LGBT people\\nEnglish mathematicians\\nGay men\\nLGBT scientists\\nScientists from London\\nSuicides by poison\\nSuicides in the United Kingdom\\n1912 births\\n1954 deaths\\nOfficers of the Order of the British Empire'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06df46",
   "metadata": {},
   "source": [
    "Make chunks of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "438d9ebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('p50k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1e804c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for getting number of tokens\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text,\n",
    "                              disallowed_special = [])\n",
    "    \n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b872b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len(\"hello I am a chunk of text and using the tiktoken_len function \"\n",
    "             \"we can find the length of this chunk of text in tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e9765cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LangChain to split the text on chunks based on number of tokens\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 400,\n",
    "                                               chunk_overlap = 20,\n",
    "                                               length_function = tiktoken_len,\n",
    "                                               separators = [\"\\n\\n\", \"\\n\", \" \", \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5a07d63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English mathematician and computer scientist. He was born in Maida Vale, London.\\n\\nEarly life and family \\nAlan Turing was born in Maida Vale, London on 23 June 1912. His father was part of a family of merchants from Scotland. His mother, Ethel Sara, was the daughter of an engineer.\\n\\nEducation \\nTuring went to St. Michael\\'s, a school at 20 Charles Road, St Leonards-on-sea, when he was five years old.\\n\"This is only a foretaste of what is to come, and only the shadow of what is going to be.” – Alan Turing.\\n\\nThe Stoney family were once prominent landlords, here in North Tipperary. His mother Ethel Sara Stoney (1881–1976) was daughter of Edward Waller Stoney (Borrisokane, North Tipperary) and Sarah Crawford (Cartron Abbey, Co. Longford); Protestant Anglo-Irish gentry.\\n\\nEducated in Dublin at Alexandra School and College; on October 1st 1907 she married Julius Mathison Turing, latter son of Reverend John Robert Turing and Fanny Boyd, in Dublin. Born on June 23rd 1912, Alan Turing would go on to be regarded as one of the greatest figures of the twentieth century.\\n\\nA brilliant mathematician and cryptographer Alan was to become the founder of modern-day computer science and artificial intelligence; designing a machine at Bletchley Park to break secret Enigma encrypted messages used by the Nazi German war machine to protect sensitive commercial, diplomatic and military communications during World War 2. Thus, Turing made the single biggest contribution to the Allied victory in the war against Nazi Germany, possibly saving the lives of an estimated 2 million people, through his effort in shortening World War II.',\n",
       " 'In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\\n\\nAlas, Turing accidentally or otherwise lost his life in 1954, having been subjected by a British court to chemical castration, thus avoiding a custodial sentence. He is known to have ended his life at the age of 41 years, by eating an apple laced with cyanide.\\n\\nCareer \\nTuring was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\\n\\nTuring was interested in artificial intelligence. He proposed the Turing test, to say when a machine could be called \"intelligent\". A computer could be said to \"think\" if a human talking with it could not tell it was a machine.\\n\\nDuring World War II, Turing worked with others to break German ciphers (secret messages). He  worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain\\'s codebreaking centre that produced Ultra intelligence.\\nUsing cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.',\n",
       " 'From 1945 to 1947, Turing worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory. He presented a paper on 19 February 1946. That paper was \"the first detailed design of a stored-program computer\". Although it was possible to build ACE, there were delays in starting the project. In late 1947 he returned to Cambridge for a sabbatical year. While he was at Cambridge, the Pilot ACE was built without him. It ran its first program on 10\\xa0May 1950.\\n\\nPrivate life \\nTuring was a homosexual man. In 1952, he admitted having had sex with a man in England. At that time, homosexual acts were illegal. Turing was convicted. He had to choose between going to jail and taking hormones to lower his sex drive. He decided to take the hormones. After his punishment, he became impotent. He also grew breasts.\\n\\nIn May 2012, a private member\\'s bill was put before the House of Lords to grant Turing a statutory pardon. In July 2013, the government supported it. A royal pardon was granted on 24 December 2013.\\n\\nDeath \\nIn 1954, Turing died from cyanide poisoning. The cyanide came from either an apple which was poisoned with cyanide, or from water that had cyanide in it. The reason for the confusion is that the police never tested the apple for cyanide. It is also suspected that he committed suicide.\\n\\nThe treatment forced on him is now believed to be very wrong. It is against medical ethics and international laws of human rights. In August 2009, a petition asking the British Government to apologise to Turing for punishing him for being a homosexual was started. The petition received thousands of signatures. Prime Minister Gordon Brown acknowledged the petition. He called Turing\\'s treatment \"appalling\".\\n\\nReferences']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the text into chunks\n",
    "chunks = text_splitter.split_text(data[6]['text'])[:3]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf73bc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 304, 370)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check lenght of chunks\n",
    "tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e75fb9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\GReyes15\\Anaconda3\\lib\\site-packages\\langchain\\embeddings\\openai.py:217: UserWarning: WARNING! document_model_name is not default parameter.\n",
      "                    document_model_name was transferred to model_kwargs.\n",
      "                    Please confirm that document_model_name is what you intended.\n",
      "  warnings.warn(\n",
      "D:\\Users\\GReyes15\\Anaconda3\\lib\\site-packages\\langchain\\embeddings\\openai.py:217: UserWarning: WARNING! query_model_name is not default parameter.\n",
      "                    query_model_name was transferred to model_kwargs.\n",
      "                    Please confirm that query_model_name is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use LangChain embedding model using OpenAI\n",
    "embed = OpenAIEmbeddings(document_model_name = 'text-embedding-ada-002',\n",
    "                         query_model_name = 'text-embedding-ada-002',\n",
    "                         openai_api_key = API_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "248dc0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedd the text\n",
    "texts = ['this is the first chunk of text',\n",
    "         'then another second chunk of text is here']\n",
    "\n",
    "res = embed.embed_documents(texts)\n",
    "\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985e203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create embedding database on Pinecone (substitue to BQ or Cloud Storage)\n",
    "index_name = 'langchain-retrieval-augmentation'\n",
    "\n",
    "pinecone.init(api_key = \"a66bd3e9-cb62-409b-8842-c2cdb5fc1a7d\",\n",
    "              environment = 'gcp-starter')\n",
    "\n",
    "# Create new index\n",
    "pinecone.create_index(name = index_name,\n",
    "                      metric = 'dotproduct',\n",
    "                      dimension = len(res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.GRPCIndex(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the embedding space\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_limit = 100\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i, record in enumerate(tqdm(data)):\n",
    "    # Get metadata fields for each record\n",
    "    metadata = {'wiki-id': str(record['id']),\n",
    "                'source': record['url'],\n",
    "                'title': record['title']}\n",
    "    \n",
    "    # Create chunks from the record text\n",
    "    record_texts = text_splitter.split_text(record['text'])\n",
    "    \n",
    "    # Create individual metadata dicts for each chunk\n",
    "    record_metadatas = [{'chunk': j,\n",
    "                         'text': text,\n",
    "                         **metadata} for j, text in enumerate(record_texts)]\n",
    "    \n",
    "    # Append the metadata to current batches\n",
    "    texts.extend(record_texts)\n",
    "    metadatas.extend(record_metadatas)\n",
    "    \n",
    "    # Add texts if reaching the batch_limit\n",
    "    if len(texts) >= batch_limit:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = embed.embed_documents(texts,\n",
    "                                       chunk_size = 16)\n",
    "        index.upsert(vectors = zip(ids, embeds, metadatas))\n",
    "        texts = []\n",
    "        metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96141aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect the embedding space with LangChain\n",
    "text_field = 'text'\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(index,\n",
    "                       embed.embed_query,\n",
    "                       text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac822dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test with similarity search\n",
    "query = 'Who was Benito Mussolini?'\n",
    "\n",
    "vectorstore.similarity_search(query,\n",
    "                              k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5025f3",
   "metadata": {},
   "source": [
    "Chain_type documentation: https://python.langchain.com/docs/modules/chains/document/stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a57606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion LLM\n",
    "llm = AzureChatOpenAI(deployment_name = 'gpt-35-turbo',\n",
    "                      model_name = 'gpt-35-turbo',\n",
    "                      temperature = 0)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = llm,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f1b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add citations to the answer\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(llm = llm,\n",
    "                                                              chain_type = \"stuff\",\n",
    "                                                              retriever = vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ffd0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_sources(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa476671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Pinecone index\n",
    "pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c87d9",
   "metadata": {},
   "source": [
    "# Chapter 6. AI Agents\n",
    "\n",
    "LangChain agents are tools that help LLM models to work on tasks that are difficutl for them to run, like online web search, or mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "29ce0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chains import LLMMathChain, LLMChain\n",
    "from langchain.agents import Tool, load_tools, initialize_agent, create_sql_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import Wikipedia, SerpAPIWrapper\n",
    "from langchain.agents.react.base import DocstoreExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "662d2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI\n",
    "path = \"API_key.txt\"\n",
    "with open(path) as f:\n",
    "    API_key = f.readlines()[0]\n",
    "    \n",
    "os.environ['OPENAI_API_KEY'] = API_key\n",
    "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
    "os.environ['OPENAI_API_BASE'] = 'azure-openai-api-url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9ac535d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base LLM model\n",
    "llm = AzureOpenAI(deployment_name = \"text-davinci-003\",\n",
    "                  model_name = \"text-davinci-003\",\n",
    "                  temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f228114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\GReyes15\\Anaconda3\\lib\\site-packages\\langchain\\chains\\llm_math\\base.py:56: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tool for the LLM model. In this case a calculator\n",
    "llm_math = LLMMathChain(llm = llm)\n",
    "\n",
    "# Initialize the math tool (deprecated, better to use it as a chain like in Chapter 3)\n",
    "math_tool = Tool(name = 'Calculator',\n",
    "                 func = llm_math.run,\n",
    "                 description = 'Useful for when you need to answer question about math.')\n",
    "\n",
    "# Pass all the tools to the LLM inside a list\n",
    "tools = [math_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "84b08f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Calculator', 'Useful for when you need to answer question about math.')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[0].name, tools[0].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b8da88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also load prebuilt agents\n",
    "tools = load_tools(['llm-math'],\n",
    "                   llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a3ca912a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Calculator', 'Useful for when you need to answer questions about math.')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[0].name, tools[0].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7777d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "zero_shot_agent = initialize_agent(agent = 'zero-shot-react-description',\n",
    "                                   tools = tools,\n",
    "                                   llm = llm,\n",
    "                                   verbose = True,\n",
    "                                   max_iterations = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "705ac039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to calculate this expression\n",
      "Action: Calculator\n",
      "Action Input: (4.5*2.1)^2.2\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 139.94261298333066\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 139.94261298333066\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is (4.5*2.1)^2.2?', 'output': '139.94261298333066'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_agent(\"what is (4.5*2.1)^2.2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a2b0e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to figure out how many apples are in the boxes\n",
      "Action: Calculator\n",
      "Action Input: 8 * 2.5\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 20.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to add the apples Mary has to the apples in the boxes\n",
      "Action: Calculator\n",
      "Action Input: 4 + 20.0\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 24.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: We have 24 apples.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'if Mary has four apples and Giorgio brings two and a half apple boxes (apple box contains eight apples), how many apples do we have?',\n",
       " 'output': 'We have 24 apples.'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_agent(\"if Mary has four apples and Giorgio brings two and a half apple \"\n",
    "                \"boxes (apple box contains eight apples), how many apples do we \"\n",
    "                \"have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4080988a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to look up the answer\n",
      "Action: Look up\n",
      "Action Input: Capital of Norway\u001b[0m\n",
      "Observation: Look up is not a valid tool, try one of [Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to look up the answer in a different way\n",
      "Action: Calculator\n",
      "Action Input: Capital of Norway\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown format from LLM: This question cannot be answered using the numexpr library, as it is not a mathematical expression.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [89]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We need a tool for every task we ask the agent\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mzero_shot_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is the capital of Norway?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    305\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    307\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    308\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    309\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    310\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:300\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    293\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    294\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    295\u001b[0m     inputs,\n\u001b[0;32m    296\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    297\u001b[0m )\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 300\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    305\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py:1141\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1141\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1149\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1150\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1151\u001b[0m         )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\agents\\agent.py:991\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m    989\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[1;32m--> 991\u001b[0m     observation \u001b[38;5;241m=\u001b[39m tool\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    992\u001b[0m         agent_action\u001b[38;5;241m.\u001b[39mtool_input,\n\u001b[0;32m    993\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    994\u001b[0m         color\u001b[38;5;241m=\u001b[39mcolor,\n\u001b[0;32m    995\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_run_kwargs,\n\u001b[0;32m    997\u001b[0m     )\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    999\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\tools\\base.py:364\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    363\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    366\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    368\u001b[0m     )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\tools\\base.py:336\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    335\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 336\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[0;32m    337\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    338\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[0;32m    339\u001b[0m     )\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ToolException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_tool_error:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\tools\\base.py:509\u001b[0m, in \u001b[0;36mTool._run\u001b[1;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[0;32m    507\u001b[0m     new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 509\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\n\u001b[0;32m    510\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    511\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    513\u001b[0m         )\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    516\u001b[0m     )\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:501\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    502\u001b[0m         _output_key\n\u001b[0;32m    503\u001b[0m     ]\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    507\u001b[0m         _output_key\n\u001b[0;32m    508\u001b[0m     ]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    305\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    307\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    308\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    309\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    310\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:300\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    293\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    294\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    295\u001b[0m     inputs,\n\u001b[0;32m    296\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    297\u001b[0m )\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 300\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    305\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\chains\\llm_math\\base.py:157\u001b[0m, in \u001b[0;36mLLMMathChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    151\u001b[0m _run_manager\u001b[38;5;241m.\u001b[39mon_text(inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key])\n\u001b[0;32m    152\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m    153\u001b[0m     question\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key],\n\u001b[0;32m    154\u001b[0m     stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```output\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    155\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(),\n\u001b[0;32m    156\u001b[0m )\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_llm_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\langchain\\chains\\llm_math\\base.py:120\u001b[0m, in \u001b[0;36mLLMMathChain._process_llm_result\u001b[1;34m(self, llm_output, run_manager)\u001b[0m\n\u001b[0;32m    118\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m llm_output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown format from LLM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: answer}\n",
      "\u001b[1;31mValueError\u001b[0m: unknown format from LLM: This question cannot be answered using the numexpr library, as it is not a mathematical expression."
     ]
    }
   ],
   "source": [
    "# We need a tool for every task we ask the agent\n",
    "zero_shot_agent(\"what is the capital of Norway?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e1294b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables = ['query'],\n",
    "                        template = \"{query}\")\n",
    "\n",
    "llm_chain = LLMChain(llm = llm,\n",
    "                     prompt = prompt)\n",
    "\n",
    "llm_tool = Tool(name = 'Language Model',\n",
    "                func = llm_chain.run,\n",
    "                description = 'Use this tool for general purpose queries and logic.')\n",
    "\n",
    "# Add the new tool\n",
    "tools.append(llm_tool)\n",
    "\n",
    "# Reinitialize the agent\n",
    "zero_shot_agent = initialize_agent(agent = 'zero-shot-react-description',\n",
    "                                   tools = tools,\n",
    "                                   llm = llm,\n",
    "                                   verbose = True,\n",
    "                                   max_iterations = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7159260d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the capital of Norway\n",
      "Action: Language Model\n",
      "Action Input: \"What is the capital of Norway?\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "\n",
      "The capital of Norway is Oslo.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The capital of Norway is Oslo.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the capital of Norway?',\n",
       " 'output': 'The capital of Norway is Oslo.'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_agent(\"what is the capital of Norway?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543641e6",
   "metadata": {},
   "source": [
    "## Agent Types - Zero Shot ReAct\n",
    "\n",
    "Basic agent to perform zero-shot tasks that do not require memory. This means that the agent considers the isngle interaction given by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abda8cb",
   "metadata": {},
   "source": [
    "### Testing Database for SQL Agent\n",
    "This example will use a SQL agent, so we need to create and connect a database for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "12c2645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData\n",
    "\n",
    "metadata_obj = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "85137a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, Table, Date, Float\n",
    "\n",
    "stocks = Table(\"stocks\",\n",
    "               metadata_obj,\n",
    "               Column(\"obs_id\", Integer, primary_key=True),\n",
    "               Column(\"stock_ticker\", String(4), nullable=False),\n",
    "               Column(\"price\", Float, nullable=False),\n",
    "               Column(\"date\", Date, nullable=False),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "23da2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7f3f7c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "observations = [\n",
    "    [1, 'ABC', 200, datetime(2023, 1, 1)],\n",
    "    [2, 'ABC', 208, datetime(2023, 1, 2)],\n",
    "    [3, 'ABC', 232, datetime(2023, 1, 3)],\n",
    "    [4, 'ABC', 225, datetime(2023, 1, 4)],\n",
    "    [5, 'ABC', 226, datetime(2023, 1, 5)],\n",
    "    [6, 'XYZ', 810, datetime(2023, 1, 1)],\n",
    "    [7, 'XYZ', 803, datetime(2023, 1, 2)],\n",
    "    [8, 'XYZ', 798, datetime(2023, 1, 3)],\n",
    "    [9, 'XYZ', 795, datetime(2023, 1, 4)],\n",
    "    [10, 'XYZ', 791, datetime(2023, 1, 5)],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "60895b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import insert\n",
    "\n",
    "def insert_obs(obs):\n",
    "    stmt = insert(stocks).values(\n",
    "    obs_id=obs[0],\n",
    "    stock_ticker=obs[1],\n",
    "    price=obs[2],\n",
    "    date=obs[3]\n",
    "    )\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "54b79ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in observations:\n",
    "    insert_obs(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "18e7d93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\GReyes15\\Anaconda3\\lib\\site-packages\\langchain_experimental\\sql\\base.py:75: UserWarning: Directly instantiating an SQLDatabaseChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "db = SQLDatabase(engine)\n",
    "sql_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9918d5a",
   "metadata": {},
   "source": [
    "### Continue with Zero Shot ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "47814f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SQL tool\n",
    "sql_tool = Tool(name = 'Stock DB',\n",
    "                func = sql_chain.run,\n",
    "                description = 'useful for when you need to answer questions about stocks and their prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "346a4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools(['llm-math'],\n",
    "                   llm = llm)\n",
    "\n",
    "# Add SQL db tool\n",
    "tools.append(sql_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8ba520b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_agent = initialize_agent(agent = 'zero-shot-react-description',\n",
    "                                   tools = tools,\n",
    "                                   llm = llm,\n",
    "                                   verbose = True,\n",
    "                                   max_iterations = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9eba0375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to compare the stock prices of ABC and XYZ on two different days\n",
      "Action: Stock DB\n",
      "Action Input: Stock prices of ABC and XYZ on January 3rd and January 4th\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "Stock prices of ABC and XYZ on January 3rd and January 4th\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT stock_ticker, price, date FROM stocks WHERE (stock_ticker = 'ABC' OR stock_ticker = 'XYZ') AND (date = '2023-01-03' OR date = '2023-01-04')\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('ABC', 232.0, '2023-01-03'), ('ABC', 225.0, '2023-01-04'), ('XYZ', 798.0, '2023-01-03'), ('XYZ', 795.0, '2023-01-04')]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe stock prices of ABC and XYZ on January 3rd and January 4th were 232.0 and 225.0 for ABC, and 798.0 and 795.0 for XYZ, respectively.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe stock prices of ABC and XYZ on January 3rd and January 4th were 232.0 and 225.0 for ABC, and 798.0 and 795.0 for XYZ, respectively.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the ratio between the two stock prices\n",
      "Action: Calculator\n",
      "Action Input: 232.0/225.0 and 798.0/795.0\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 1.0037735849056604\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The multiplication of the ratio between stock prices for 'ABC' and 'XYZ' in January 3rd and the ratio between the same stock prices in January the 4th is 1.0037735849056604.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = zero_shot_agent(\"\"\"What is the multiplication of the ratio between stock prices\n",
    "for 'ABC' and 'XYZ' in January 3rd and the ratio between the\n",
    "same stock prices in January the 4th?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "967df100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "Stock DB: useful for when you need to answer questions about stocks and their prices\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Calculator, Stock DB]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "# Look at the prompt of the agent\n",
    "print(zero_shot_agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc937ed8",
   "metadata": {},
   "source": [
    "## Conversational ReAct\n",
    "\n",
    "Agent with memory. Useful for use cases that need to remember previous interations in a conversation or interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c263641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the memory buffer\n",
    "memory = ConversationBufferMemory(memory_key = 'chat_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0d9b8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent with the memory buffer\n",
    "conversational_agent = initialize_agent(agent = 'conversational-react-description',\n",
    "                                        tools = tools,\n",
    "                                        llm = llm,\n",
    "                                        verbose = True,\n",
    "                                        max_iterations = 3,\n",
    "                                        memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2752c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Stock DB\n",
      "Action Input: ABC on January the 1st\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "ABC on January the 1st\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-01'\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(200.0,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe price of ABC on January the 1st was 200.0.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe price of ABC on January the 1st was 200.0.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: Is there anything else I can help you with?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = conversational_agent(\"Provide me with the stock prices for ABC on January the 1st\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fe3986d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Stock DB\n",
      "Action Input: Stock prices for XYZ on January 1st\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "Stock prices for XYZ on January 1st\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT price FROM stocks WHERE stock_ticker = 'XYZ' AND date = '2023-01-01' LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(810.0,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe stock price for XYZ on January 1st was 810.0.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe stock price for XYZ on January 1st was 810.0.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: Is there anything else I can help you with?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Ask again without specifying the date\n",
    "result = conversational_agent(\"What are the stock prices for XYZ on the same day?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a911fdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant is a large language model trained by OpenAI.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
      "\n",
      "TOOLS:\n",
      "------\n",
      "\n",
      "Assistant has access to the following tools:\n",
      "\n",
      "> Calculator: Useful for when you need to answer questions about math.\n",
      "> Stock DB: useful for when you need to answer questions about stocks and their prices\n",
      "\n",
      "To use a tool, please use the following format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: the action to take, should be one of [Calculator, Stock DB]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "{chat_history}\n",
      "\n",
      "New input: {input}\n",
      "{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "# How does it uses the memory?\n",
    "print(conversational_agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fba9640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Stock DB\n",
      "Action Input: Get the ratio between stock prices for 'ABC' and 'XYZ' in January 3rd and the ratio between the same stock prices in January the 4th\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "Get the ratio between stock prices for 'ABC' and 'XYZ' in January 3rd and the ratio between the same stock prices in January the 4th\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT (SELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-03') / (SELECT price FROM stocks WHERE stock_ticker = 'XYZ' AND date = '2023-01-03') AS ratio_jan_3, (SELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-04') / (SELECT price FROM stocks WHERE stock_ticker = 'XYZ' AND date = '2023-01-04') AS ratio_jan_4 FROM stocks LIMIT 1;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(0.2907268170426065, 0.2830188679245283)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe ratio between stock prices for 'ABC' and 'XYZ' in January 3rd is 0.2907268170426065 and the ratio between the same stock prices in January the 4th is 0.2830188679245283.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe ratio between stock prices for 'ABC' and 'XYZ' in January 3rd is 0.2907268170426065 and the ratio between the same stock prices in January the 4th is 0.2830188679245283.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: The multiplication of the ratio between stock prices for 'ABC' and 'XYZ' in January 3rd and the ratio between the same stock prices in January the 4th is 0.08254545454545455.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# This agent is built for conversational purposes, and uses more difficult approaches for multi-step procedures\n",
    "result = conversational_agent(\"\"\"What is the multiplication of the ratio between stock prices\n",
    "for 'ABC' and 'XYZ' in January 3rd and the ratio between the\n",
    "same stock prices in January the 4th?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4658414",
   "metadata": {},
   "source": [
    "## ReAct Docstore\n",
    "\n",
    "Agent built for information search and lookup. It allows to store and retrieve information using different retrieval methods, like Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "898273b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore = DocstoreExplorer(Wikipedia())\n",
    "\n",
    "tools = [Tool(name = 'Search',\n",
    "              func = docstore.search,\n",
    "              description = 'Search Wikipedia'),\n",
    "         Tool(name = 'Lookup',\n",
    "              func = docstore.lookup,\n",
    "              description = 'Lookup a term in Wikipedia')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5befd6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "docstore_agent = initialize_agent(agent = 'react-docstore',\n",
    "                                  tools = tools,\n",
    "                                  llm = llm,\n",
    "                                  verbose = True,\n",
    "                                  max_iterations = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "59bee9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to search Archimedes and find his last words.\n",
      "Action: Search[Archimedes]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mArchimedes of Syracuse (, ARK-ihm-EE-deez; c. 287 – c. 212 BC) was an Ancient Greek mathematician, physicist, engineer, astronomer, and inventor from the ancient city of Syracuse in Sicily. Although few details of his life are known, he is regarded as one of the leading scientists in classical antiquity. Considered the greatest mathematician of ancient history, and one of the greatest of all time, Archimedes anticipated modern calculus and analysis by applying the concept of the infinitely small and the method of exhaustion to derive and rigorously prove a range of geometrical theorems. These include the area of a circle, the surface area and volume of a sphere, the area of an ellipse, the area under a parabola, the volume of a segment of a paraboloid of revolution, the volume of a segment of a hyperboloid of revolution, and the area of a spiral.Archimedes' other mathematical achievements include deriving an approximation of pi, defining and investigating the Archimedean spiral, and devising a system using exponentiation for expressing very large numbers. He was also one of the first to apply mathematics to physical phenomena, working on statics and hydrostatics. Archimedes' achievements in this area include a proof of the law of the lever, the widespread use of the concept of center of gravity, and the enunciation of the law of buoyancy or Archimedes' principle. He is also credited with designing innovative machines, such as his screw pump, compound pulleys, and defensive war machines to protect his native Syracuse from invasion.\n",
      "Archimedes died during the siege of Syracuse, when he was killed by a Roman soldier despite orders that he should not be harmed. Cicero describes visiting Archimedes' tomb, which was surmounted by a sphere and a cylinder that Archimedes requested be placed there to represent his mathematical discoveries.\n",
      "Unlike his inventions, Archimedes' mathematical writings were little known in antiquity. Mathematicians from Alexandria read and quoted him, but the first comprehensive compilation was not made until c. 530 AD by Isidore of Miletus in Byzantine Constantinople, while commentaries on the works of Archimedes by Eutocius in the 6th century opened them to wider readership for the first time. The relatively few copies of Archimedes' written work that survived through the Middle Ages were an influential source of ideas for scientists during the Renaissance and again in the 17th century, while the discovery in 1906 of previously lost works by Archimedes in the Archimedes Palimpsest has provided new insights into how he obtained mathematical results.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The paragraph does not mention Archimedes' last words. I need to look up \"last words\".\n",
      "Action: Lookup[last words]\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m(Result 1/1) \n",
      "== Biography ==\n",
      "Archimedes was born c. 287 BC in the seaport city of Syracuse, Sicily, at that time a self-governing colony in Magna Graecia. The date of birth is based on a statement by the Byzantine Greek historian John Tzetzes that Archimedes lived for 75 years before his death in 212 BC. In the Sand-Reckoner, Archimedes gives his father's name as Phidias, an astronomer about whom nothing else is known. A biography of Archimedes was written by his friend Heracleides, but this work has been lost, leaving the details of his life obscure. It is unknown, for instance, whether he ever married or had children, or if he ever visited Alexandria, Egypt, during his youth. From his surviving written works, it is clear that he maintained collegiate relations with scholars based there, including his friend Conon of Samos and the head librarian Eratosthenes of Cyrene.The standard versions of Archimedes' life were written long after his death by Greek and Roman historians. The earliest reference to Archimedes occurs in The Histories by Polybius (c. 200–118 BC), written about 70 years after his death. It sheds little light on Archimedes as a person, and focuses on the war machines that he is said to have built in order to defend the city from the Romans. Polybius remarks how, during the Second Punic War, Syracuse switched allegiances from Rome to Carthage, resulting in a military campaign under the command of Marcus Claudius Marcellus and Appius Claudius Pulcher, who besieged the city from 213 to 212 BC. He notes that the Romans underestimated Syracuse's defenses, and mentions several machines Archimedes designed, including improved catapults, crane-like machines that could be swung around in an arc, and other stone-throwers. Although the Romans ultimately captured the city, they suffered considerable losses due to Archimedes' inventiveness.Cicero (106–43 BC) mentions Archimedes in some of his works. While serving as a quaestor in Sicily, Cicero found what was presumed to be Archimedes' tomb near the Agrigentine gate in Syracuse, in a neglected condition and overgrown with bushes. Cicero had the tomb cleaned up and was able to see the carving and read some of the verses that had been added as an inscription. The tomb carried a sculpture illustrating Archimedes' favorite mathematical proof, that the volume and surface area of the sphere are two-thirds that of an enclosing cylinder including its bases. He also mentions that Marcellus brought to Rome two planetariums Archimedes built. The Roman historian Livy (59 BC–17 AD) retells Polybius' story of the capture of Syracuse and Archimedes' role in it.\n",
      "Plutarch (45–119 AD) wrote in his Parallel Lives that Archimedes was related to King Hiero II, the ruler of Syracuse. He also provides at least two accounts on how Archimedes died after the city was taken. According to the most popular account, Archimedes was contemplating a mathematical diagram when the city was captured. A Roman soldier commanded him to come and meet Marcellus, but he declined, saying that he had to finish working on the problem. This enraged the soldier, who killed Archimedes with his sword. Another story has Archimedes carrying mathematical instruments before being killed because a soldier thought they were valuable items. Marcellus was reportedly angered by Archimedes' death, as he considered him a valuable scientific asset (he called Archimedes \"a geometrical Briareus\") and had ordered that he should not be harmed.The last words attributed to Archimedes are \"Do not disturb my circles\" (Latin, \"Noli turbare circulos meos\"; Katharevousa Greek, \"μὴ μου τοὺς κύκλους τάραττε\"), a reference to the mathematical drawing that he was supposedly studying when disturbed by the Roman soldier. There is no reliable evidence that Archimedes uttered these words and they do not appear in Plutarch's account. A similar quotation is found in the work of Valerius Maximus (fl. 30 AD), who wrote in Memorable Doings and Sayings, \"... sed protecto manibus puluere 'noli' inquit, 'obsecro, istum disturbare'\" (\"... but protecting the dust with his hands, said 'I beg of you, do not disturb this'\").\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The paragraph mentions that the last words attributed to Archimedes are \"Do not disturb my circles\". So the answer is \"Do not disturb my circles\".\n",
      "Action: Finish[Do not disturb my circles]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What were Archimedes' last words?\",\n",
       " 'output': 'Do not disturb my circles'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstore_agent(\"What were Archimedes' last words?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d54957",
   "metadata": {},
   "source": [
    "## Self-Ask with Search\n",
    "\n",
    "Agent that performs searches and asks follow-up questions as often as required to get a final answer.\n",
    "\n",
    "This agent requires a SerpAPI key. Currently using **personal free API key** for testing purposes. Limited to **100 searches/month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "14ab5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the search chain\n",
    "path = \"SerpAPI_key.txt\"\n",
    "with open(path) as f:\n",
    "    SerpAPI_key = f.readlines()[0]\n",
    "\n",
    "search = SerpAPIWrapper(serpapi_api_key = SerpAPI_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fe8bf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the search tool\n",
    "tools = [Tool(name = 'Intermediate Answer',\n",
    "              func = search.run,\n",
    "              description = 'Google search')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2dfb85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "self_ask_with_search = initialize_agent(agent = 'self-ask-with-search',\n",
    "                                  tools = tools,\n",
    "                                  llm = llm,\n",
    "                                  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2b09058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Yes.\n",
      "Follow up: How old was Plato when he died?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3meighty\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFollow up: How old was Socrates when he died?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mapproximately 71\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFollow up: How old was Aristotle when he died?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3m62 years\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mSo the final answer is: Plato\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who lived longer: Plato, Socrates, or Aristotle?',\n",
       " 'output': 'Plato'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_ask_with_search('Who lived longer: Plato, Socrates, or Aristotle?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b74043e",
   "metadata": {},
   "source": [
    "# Chapter 7. Build Custom Tools for LLM Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "41728e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "from math import pi, sqrt, cos, sin\n",
    "from typing import Union\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import initialize_agent\n",
    "from typing import Optional\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "997c98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI\n",
    "path = \"API_key.txt\"\n",
    "with open(path) as f:\n",
    "    API_key = f.readlines()[0]\n",
    "    \n",
    "os.environ['OPENAI_API_KEY'] = API_key\n",
    "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
    "os.environ['OPENAI_API_BASE'] = 'azure-openai-api-url'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a422c1",
   "metadata": {},
   "source": [
    "## Tools with Single Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0d9d2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tool that calculates a circle circumference\n",
    "class CircumferenceTool(BaseTool):\n",
    "    name = 'Circumference calculator'\n",
    "    description = \"Use this tool when you need to calculate a circumference using the radius of a circle\"\n",
    "    \n",
    "    def _run(self, radius: Union[int, float]):\n",
    "        return float(radius)*2.0*pi\n",
    "    \n",
    "    # Asynchronous run (not covered in the chapter)\n",
    "    def _arun(self, radius: int):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f2d62aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = AzureChatOpenAI(deployment_name = 'gpt-35-turbo',\n",
    "                      model_name = 'gpt-35-turbo',\n",
    "                      temperature = 0)\n",
    "\n",
    "# Initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(memory_key = 'chat_history',\n",
    "                                                       k = 5,\n",
    "                                                       return_messages = True)\n",
    "\n",
    "# Initizalize the agent\n",
    "tools = [CircumferenceTool()]\n",
    "\n",
    "agent = initialize_agent(agent = 'chat-conversational-react-description',\n",
    "                         tools = tools,\n",
    "                         llm = llm,\n",
    "                         verbose = True,\n",
    "                         max_iterations = 3,\n",
    "                         early_stopping_method = 'generate',\n",
    "                         memory = conversational_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "40e2b2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The circumference of the circle is approximately 49.03mm.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the circumference of a circle that has a radius of 7.81mm',\n",
       " 'chat_history': [],\n",
       " 'output': 'The circumference of the circle is approximately 49.03mm.'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current agent ignores the Circumference tool\n",
    "agent(\"Calculate the circumference of a circle that has a radius of 7.81mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8d670c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant is a large language model trained by OpenAI.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n"
     ]
    }
   ],
   "source": [
    "print(agent.agent.llm_chain.prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9b5951eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the prompt to determine the agent to use tools for doing math\n",
    "sys_msg = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Unfortunately, Assistant is terrible at maths. When provided with math questions, no matter how simple, assistant always refers to it's trusty tools and absolutely does NOT try to answer math questions by itself\n",
    "\n",
    "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "70282122",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = agent.agent.create_prompt(system_message = sys_msg,\n",
    "                                       tools = tools)\n",
    "\n",
    "agent.agent.llm_chain.prompt = new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "782d78c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Circumference calculator\",\n",
      "    \"action_input\": \"7.81\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m49.071677249072565\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The circumference of a circle with a radius of 7.81mm is approximately 49.07mm.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the circumference of a circle that has a radius of 7.81mm',\n",
       " 'chat_history': [HumanMessage(content='Calculate the circumference of a circle that has a radius of 7.81mm'),\n",
       "  AIMessage(content='The circumference of the circle is approximately 49.03mm.')],\n",
       " 'output': 'The circumference of a circle with a radius of 7.81mm is approximately 49.07mm.'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"Calculate the circumference of a circle that has a radius of 7.81mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820ed32",
   "metadata": {},
   "source": [
    "## Tools with Multiple Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f63443bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a hypotenuse calculator given a combination of triangle side lenghts and/or angles\n",
    "desc = (\"\"\"Use this tool when you need to calculate the length of a hypotenuse\n",
    "given one or two sides of a triangle and/or an angle (in degrees).\n",
    "To use the tool, you must provide at least two of the following parameters:\n",
    "['adjacent_side', 'opposite_side', 'angle'].\"\"\")\n",
    "\n",
    "class PythagorasTool(BaseTool):\n",
    "    name = 'Hypotenuse calculator'\n",
    "    description = desc\n",
    "    \n",
    "    def _run(self,\n",
    "             adjacent_side: Optional[Union[int, float]] = None,\n",
    "             opposite_side: Optional[Union[int, float]] = None,\n",
    "             angle: Optional[Union[int, float]] = None):\n",
    "        \n",
    "        # Check for the given values\n",
    "        if adjacent_side and opposite_side:\n",
    "            return sqrt(float(adjacent_side)**2 + float(opposite_side)**2)\n",
    "        elif adjacent_side and angle:\n",
    "            return adjacent_side / cos(float(angle))\n",
    "        elif opposite_side and angle:\n",
    "            return opposite_side / sin(float(angle))\n",
    "        else:\n",
    "            return \"Could not calculate the hypotenuse of the triangle. Need two or more of `adjacent_side`, `opposite_side`, or `angle`.\"\n",
    "        \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "        \n",
    "tools = [PythagorasTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b5b464ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the agent's prompt\n",
    "new_prompt = agent.agent.create_prompt(system_message = sys_msg,\n",
    "                                       tools = tools)\n",
    "\n",
    "agent.agent.llm_chain.prompt = new_prompt\n",
    "\n",
    "# Update also the tools\n",
    "agent.tools = tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b2ff6190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Hypotenuse calculator\",\n",
      "    \"action_input\": {\n",
      "        \"adjacent_side\": 51,\n",
      "        \"opposite_side\": 34\n",
      "    }\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m61.29437168288782\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The length of the hypotenuse is approximately 61.29cm.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'If I have a triangle with two sides of length 51cm and 34cm, what is the length of the hypotenuse?',\n",
       " 'chat_history': [HumanMessage(content='Calculate the circumference of a circle that has a radius of 7.81mm'),\n",
       "  AIMessage(content='The circumference of the circle is approximately 49.03mm.'),\n",
       "  HumanMessage(content='Calculate the circumference of a circle that has a radius of 7.81mm'),\n",
       "  AIMessage(content='The circumference of a circle with a radius of 7.81mm is approximately 49.07mm.')],\n",
       " 'output': 'The length of the hypotenuse is approximately 61.29cm.'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"If I have a triangle with two sides of length 51cm and 34cm, what is the length of the hypotenuse?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5780cb3",
   "metadata": {},
   "source": [
    "## Create captions for online images\n",
    "\n",
    "Create a new tool based on HuggingFace code for image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dbb4a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model\n",
    "hf_model = \"Salesforce/blip-image-captioning-large\"\n",
    "\n",
    "# Use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Preprocessor will prepare images for the model\n",
    "processor = BlipProcessor.from_pretrained(hf_model)\n",
    "\n",
    "# Initialize the LLM model\n",
    "model = BlipForConditionalGeneration.from_pretrained(hf_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0e37596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = (\"\"\"Use this tool when given the URL of an image that you'd like to be\n",
    "described. It will retrun a simple caption describing the image.\"\"\")\n",
    "\n",
    "class ImageCaptionTool(BaseTool):\n",
    "    name = 'Image captioner'\n",
    "    description = desc\n",
    "    \n",
    "    def _run(self, url: str):\n",
    "        # Download the image and convert to PIL object\n",
    "        image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "        \n",
    "        # Preprocess the image\n",
    "        inputs = processor(image, return_tensors = 'pt').to(device)\n",
    "        \n",
    "        # Generate the caption\n",
    "        out = model.generate(**inputs, max_new_tokens=20)\n",
    "        \n",
    "        # Get the caption\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        \n",
    "        return caption\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise notImplementedError(\"This tool does not support async\")\n",
    "        \n",
    "tools = [ImageCaptionTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ff837927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize the agent prompt\n",
    "sys_msg = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "Overall, Assistant is a poerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\"\"\"\n",
    "\n",
    "new_prompt = agent.agent.create_prompt(system_message = sys_msg,\n",
    "                                       tools = tools)\n",
    "agent.agent.llm_chain.prompt = new_prompt\n",
    "\n",
    "# Update the agent's tools\n",
    "agent.tools = tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "27fd8e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Image captioner\",\n",
      "    \"action_input\": \"https://image.adsoftheworld.com/56agha7b3d768gu5e37rl4ji08pz\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mthere is a figurine of a dog on a plate with a knife and fork\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The image shows a figurine of a dog on a plate with a knife and fork.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What does this image show?\\nhttps://image.adsoftheworld.com/56agha7b3d768gu5e37rl4ji08pz',\n",
       " 'chat_history': [HumanMessage(content='Calculate the circumference of a circle that has a radius of 7.81mm'),\n",
       "  AIMessage(content='The circumference of the circle is approximately 49.03mm.'),\n",
       "  HumanMessage(content='Calculate the circumference of a circle that has a radius of 7.81mm'),\n",
       "  AIMessage(content='The circumference of a circle with a radius of 7.81mm is approximately 49.07mm.'),\n",
       "  HumanMessage(content='If I have a triangle with two sides of length 51cm and 34cm, what is the length of the hypotenuse?'),\n",
       "  AIMessage(content='The length of the hypotenuse is approximately 61.29cm.')],\n",
       " 'output': 'The image shows a figurine of a dog on a plate with a knife and fork.'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tool\n",
    "img_url = \"https://image.adsoftheworld.com/56agha7b3d768gu5e37rl4ji08pz\"\n",
    "agent(f\"What does this image show?\\n{img_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13e887",
   "metadata": {},
   "source": [
    "# Chapter 8. Agents with Long-Term-Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6e3aa22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "\n",
    "from datasets import load_dataset\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e14cd6",
   "metadata": {},
   "source": [
    "## Build the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "47d15930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (D:/Users/GReyes15/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset('squad', split='train')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "39a803c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5733bf84d058e614000b61be</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>As at most other universities, Notre Dame's st...</td>\n",
       "      <td>When did the Scholastic Magazine of Notre dame...</td>\n",
       "      <td>{'text': ['September 1876'], 'answer_start': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5733bed24776f41900661188</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>The university is the major seat of the Congre...</td>\n",
       "      <td>Where is the headquarters of the Congregation ...</td>\n",
       "      <td>{'text': ['Rome'], 'answer_start': [119]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5733a6424776f41900660f51</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>The College of Engineering was established in ...</td>\n",
       "      <td>How many BS level degrees are offered in the C...</td>\n",
       "      <td>{'text': ['eight'], 'answer_start': [487]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5733a70c4776f41900660f64</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>All of Notre Dame's undergraduate students are...</td>\n",
       "      <td>What entity provides help with the management ...</td>\n",
       "      <td>{'text': ['Learning Resource Center'], 'answer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id                     title  \\\n",
       "0   5733be284776f41900661182  University_of_Notre_Dame   \n",
       "5   5733bf84d058e614000b61be  University_of_Notre_Dame   \n",
       "10  5733bed24776f41900661188  University_of_Notre_Dame   \n",
       "15  5733a6424776f41900660f51  University_of_Notre_Dame   \n",
       "20  5733a70c4776f41900660f64  University_of_Notre_Dame   \n",
       "\n",
       "                                              context  \\\n",
       "0   Architecturally, the school has a Catholic cha...   \n",
       "5   As at most other universities, Notre Dame's st...   \n",
       "10  The university is the major seat of the Congre...   \n",
       "15  The College of Engineering was established in ...   \n",
       "20  All of Notre Dame's undergraduate students are...   \n",
       "\n",
       "                                             question  \\\n",
       "0   To whom did the Virgin Mary allegedly appear i...   \n",
       "5   When did the Scholastic Magazine of Notre dame...   \n",
       "10  Where is the headquarters of the Congregation ...   \n",
       "15  How many BS level degrees are offered in the C...   \n",
       "20  What entity provides help with the management ...   \n",
       "\n",
       "                                              answers  \n",
       "0   {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "5   {'text': ['September 1876'], 'answer_start': [...  \n",
       "10          {'text': ['Rome'], 'answer_start': [119]}  \n",
       "15         {'text': ['eight'], 'answer_start': [487]}  \n",
       "20  {'text': ['Learning Resource Center'], 'answer...  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "data = data.to_pandas()\n",
    "data.drop_duplicates(subset='context', keep='first', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f147347",
   "metadata": {},
   "source": [
    "### Initialize Embedding Model and vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d4c103ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI\n",
    "path = \"API_key.txt\"\n",
    "with open(path) as f:\n",
    "    API_key = f.readlines()[0]\n",
    "    \n",
    "os.environ['OPENAI_API_KEY'] = API_key\n",
    "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
    "os.environ['OPENAI_API_BASE'] = 'azure-openai-api-url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4d684c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\GReyes15\\Anaconda3\\lib\\site-packages\\langchain\\embeddings\\openai.py:217: UserWarning: WARNING! document_model_name is not default parameter.\n",
      "                    document_model_name was transferred to model_kwargs.\n",
      "                    Please confirm that document_model_name is what you intended.\n",
      "  warnings.warn(\n",
      "D:\\Users\\GReyes15\\Anaconda3\\lib\\site-packages\\langchain\\embeddings\\openai.py:217: UserWarning: WARNING! query_model_name is not default parameter.\n",
      "                    query_model_name was transferred to model_kwargs.\n",
      "                    Please confirm that query_model_name is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use LangChain embedding model using OpenAI\n",
    "embed = OpenAIEmbeddings(document_model_name = 'text-embedding-ada-002',\n",
    "                         query_model_name = 'text-embedding-ada-002',\n",
    "                         openai_api_key = API_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4113cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create embedding database on Pinecone (substitue to BQ or Cloud Storage)\n",
    "index_name = 'langchain-retrieval-agent'\n",
    "\n",
    "pinecone.init(api_key = \"pinecone-api-key\",\n",
    "              environment = 'gcp-starter')\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # Create the new index\n",
    "    pinecone.create_index(name = index_name,\n",
    "                          metric = 'dotproduct',\n",
    "                          dimension = 1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60756214",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.GRPCIndex(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2d152",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45157baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the embedding space\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_size = 100\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    # Get end of batch\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    batch = data.iloc[i:i_end]\n",
    "    \n",
    "    # Get metadata for this record\n",
    "    metadatas = [{'title': record['title'],\n",
    "                  'text': record['context']} for j, record in batch.iterrows()]\n",
    "    \n",
    "    # Get the list of contexts/documents\n",
    "    documents = batch['context']\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeds = embed.embed_documents(documents,\n",
    "                                   chunk_size = 16)\n",
    "    \n",
    "    # Get IDS\n",
    "    ids = batch['id']\n",
    "    \n",
    "    # Add to pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aec899",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb84874",
   "metadata": {},
   "source": [
    "## Creating a Vector Store and Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = 'text'\n",
    "\n",
    "# Switch back to normal index for Langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(index,\n",
    "                       embed.embed_query,\n",
    "                       text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607bc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use similarity search for testing semantic search\n",
    "query = 'when was the college of engineering in the University of Notre Dame established?'\n",
    "\n",
    "vectorstore.similarity_search(query,\n",
    "                              k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa22b5",
   "metadata": {},
   "source": [
    "## Initialize Conversational Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140585a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = AzureChatOpenAI(deployment_name = 'gpt-35-turbo',\n",
    "                      model_name = 'gpt-35-turbo',\n",
    "                      temperature = 0)\n",
    "\n",
    "# Initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(memory_key = 'chat_history',\n",
    "                                                       k = 5,\n",
    "                                                       return_messages = True)\n",
    "\n",
    "# Retreival QA chain\n",
    "qa = RetrievalQA.from_chain_type(llm = llm,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the previous query\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f30f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the chain into a Tool\n",
    "tools = [Tool(name = 'Knowledge Base',\n",
    "              func = qa.run,\n",
    "              description = \"Use this tool when answering general knowledge queries to get more information about the topic.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e213969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "agent = initialize_agent(agent = 'chat-conversational-react-description',\n",
    "                         tools = tools,\n",
    "                         llm = llm,\n",
    "                         verbose = True,\n",
    "                         max_iterations = 3,\n",
    "                         early_stopping_method = 'generate',\n",
    "                         memory = conversational_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8538f",
   "metadata": {},
   "source": [
    "### Using the Conversational Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3200f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"what is 2 * 7?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"can you tell me some facts about the University of Notre Dame?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64864b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"can you summarize these facts in two short sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e043c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Pinecone index\n",
    "pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbbad9",
   "metadata": {},
   "source": [
    "# Chapter 9. Langchain for GCP Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e272ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.oauth2 import service_account\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c41aee5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize connection to GCP VertexAI\n",
    "credentials = service_account.Credentials.from_service_account_file('gcp-labx-aba-461f9042302b.json')\n",
    "\n",
    "aiplatform.init(project = 'gcp-labx-aba',\n",
    "                credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13321700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = VertexAI(model_name = 'text-bison@001',\n",
    "               temperature = 0)\n",
    "\n",
    "# Initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(memory_key = 'chat_history',\n",
    "                                                       k = 5,\n",
    "                                                       return_messages = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7da4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initizalize the tools (math and image caption)\n",
    "tools = load_tools(['llm-math'],\n",
    "                   llm = llm)\n",
    "\n",
    "hf_model = \"Salesforce/blip-image-captioning-large\"\n",
    "device = 'cpu'\n",
    "\n",
    "# preprocessor will prepare images for the model\n",
    "processor = BlipProcessor.from_pretrained(hf_model)\n",
    "# then we initialize the model itself\n",
    "model = BlipForConditionalGeneration.from_pretrained(hf_model).to(device)\n",
    "\n",
    "desc = (\"\"\"Use this tool when given the URL of an image that you'd like to be described.\n",
    "It will return a simple caption describing the image.\"\"\")\n",
    "\n",
    "class ImageCaptionTool(BaseTool):\n",
    "    name = \"Image captioner\"\n",
    "    description = desc\n",
    "    \n",
    "    def _run(self, url: str):\n",
    "        # download the image and convert to PIL object\n",
    "        image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "        # preprocess the image\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        # generate the caption\n",
    "        out = model.generate(**inputs, max_new_tokens=20)\n",
    "        # get the caption\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "tools.append(ImageCaptionTool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "agent = initialize_agent(agent = 'chat-conversational-react-description',\n",
    "                         tools = tools,\n",
    "                         llm = llm,\n",
    "                         verbose = True,\n",
    "                         max_iterations = 3,\n",
    "                         early_stopping_method = 'generate',\n",
    "                         memory = conversational_memory)\n",
    "\n",
    "# Make sure the agent uses the math tool\n",
    "sys_msg = \"\"\"Assistant is a large language model trained by VertexAI.\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "Unfortunately, Assistant is terrible at maths. When provided with math questions, no matter how simple, assistant always refers to it's trusty tools and absolutely does NOT try to answer math questions by itself\n",
    "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\"\"\"\n",
    "\n",
    "new_prompt = agent.agent.create_prompt(system_message = sys_msg,\n",
    "                                       tools=tools)\n",
    "\n",
    "agent.agent.llm_chain.prompt = new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"How much is 8^16?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://image.adsoftheworld.com/56agha7b3d768gu5e37rl4ji08pz\"\n",
    "agent(f\"What does this image show?\\n{img_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
